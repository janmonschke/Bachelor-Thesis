### SEO
Search Engine Optimization is very important for modern websites to get a good ranking in search results from Google or any other search engine. Search engines build their indexes with so called Web crawlers [http://en.wikipedia.org/wiki/Web_crawler] that process the contents of websites to get an understanding to what topics they are related. Web crawlers automatically follow links on web pages to create relations between websites and to find out the importance of websites by counting the links that link back to a certain page.
They are built to rapidly crawl through many websites, which means that the basic crawlers neither load images nor CSS-, nor JavaScript files to decrease the loading time. This has a negative impact on SPWAs because the content would not be correctly indexed or even not indexed at all because the client-side JavaScript-based URL-router would not get started when a Web crawler is on the website, since they don't execute JavaScript.
Furthermore, if the start page of a SPWA was generated by the JavaScript templating system, the page wouldn't even get added to any search engine index because the crawler would just see a blank HTML page. This would make SPWAs for now unusable for projects that need to have a good ranking in search engines. But there are several ways to go around this problem.

Google proposed a technique that let's their crawler index a SPWA [http://code.google.com/intl/de/web/ajaxcrawling/docs/getting-started.html]. When their crawler finds a URL in the SPWA-typical "#!"-style it will request a special URL on the website's server that should return a HTML snapshot of the requested page. This snapshot should represent the content of the page that should get indexed. As an example, a request to the URL "mydomain.tld/#!/test" would create a Web crawler request to "mydomain.tld/?_escaped_fragment_=test" and the server should respond with the HTML snapshot.
This solution can easily lead to a lot of duplicated code since a router is needed in the backend that has to work exactly the same as  the router in the frontend. Additionally, often view code might get duplicated because templates have to get maintained in the frontend and in the backend.
One thing to keep in mind with this technique is that currently only the Google Web crawler supports the advanced URL scheme and none of the other competitors such as Bing and Yahoo do.

Another technique works especially well for community pages where there is a difference between the site a user sees when he is logged in and the site he sees when he is not logged in (Web crawlers normally don't log in to communities). In that case all public pages could get served from the backend so that Web crawlers easily can index them. Internal pages would then get served as SPWA since these pages should not get indexed anyway. The extra effort that is needed for this technique is reasonable since only few pages (e.g. index, about, pricing, contact) need a backend view and most of the client-side code doesn't need to get duplicated.